# Transformer from Scratch

Implementation of a vanilla Transformer from scratch using only numpy. The primary objective is to investigate whether large language models built on artificial neural networks can support consciousness. 

## Table of Contents

- [Introduction](#introduction)
- [Motivation](#motivation)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Experiments](#experiments)
- [Contributing](#contributing)

## Introduction

This project demonstrates a foundational implementation of the Transformer architecture solely using `numpy`, a numerical computing library in Python. Transformers have revolutionized natural language processing by enabling models like GPT and BERT, which can perform tasks ranging from translation to text generation. 

## Motivation

The central question driving this project is: **Can LLMs built on ANNs support consciousness?** While the concept of consciousness in machines is still theoretical, this project aims to address the computational capabilities of LLMs by constructing them from the ground up. The focus is on understanding the relationship between the structure of Transformers and the possibility of emergent properties, such as higher cognitive function in the brain.

## Project Structure

The project is organized as follows:

├── `data/`          : Dataset used for training and testing  
├── `models/`        : Transformer model implementations  
├── `experiments/`   : Experiment scripts and results  
├── `.ipynb`     : Jupyter notebooks for notes, exploration and analysis  
├── `utils/`         : Utility functions for data processing, etc.  
└──  `tests/`         : Unit tests for the implementation  



## Installation

To get started with this project, clone the repository and install the necessary dependencies:

```bash
git clone https://github.com/DorsaRoh/transformer-from-scratch.git
cd transformer-from-scratch
```

```bash
pip install numpy
```


## Contributing

Contributions to this project are welcome. Whether it's improving the existing codebase, adding new features, or conducting new experiments, your input is valuable!
