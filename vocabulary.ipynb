{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "\n",
    "a summary of important vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "\n",
    "- `token`: a single unit of information (word, character, audio, pixel, etc)\n",
    "    - ex. Word level example, \"I need coffee before I write\", would be ['I', 'need', 'coffee', 'before', 'I', 'write'].\n",
    "    - ex. Punctuation or special chars example \"Coffee is Life!!!\" would be ['Coffee', 'is', 'life', '!', '!', '!'].\n",
    "- `tokenisation`: breaking down a piece of text into individual tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "- `Embedding matrix`: matrix that maps each word in the training data to a vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention Mechanism\n",
    "\n",
    "- `Context window`: specified range of tokens surrounding a target token within a text or sequence\n",
    "    - the relationship of this target token to the others around it dictates its meaning\n",
    "    - limits how much text the transformer can incorporate when its making a prediction of the next word\n",
    "    <br><br>\n",
    "    - `Context size`: the number of tokens considered on each side of the target token \n",
    "        - ex. context window size of 20 means 20 tokens to the left and 20 tokens to the right of the target token\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
